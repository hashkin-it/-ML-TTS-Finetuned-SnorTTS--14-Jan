{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522a3d40-019f-465e-8e35-c86747ba42d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers datasets accelerate peft bitsandbytes soundfile librosa\n",
    "!pip install snac\n",
    "# Force update the three libraries that must work together\n",
    "# !pip install -U bitsandbytes accelerate transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2c19d86-5593-4695-a51a-47ead984bb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported!\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 2: Import Libraries\n",
    "# ============================================================================\n",
    "# WHY: We need to load all the tools we just installed\n",
    "# WHAT IT DOES: Brings in all the functions and classes we'll use\n",
    "# Think of this like opening your toolbox before starting work\n",
    "import librosa\n",
    "import torch  # PyTorch - the foundation for deep learning\n",
    "import soundfile as sf  # For saving audio files\n",
    "import numpy as np  # For working with numbers and arrays\n",
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    AutoFeatureExtractor,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from datasets import load_dataset, Audio  # For loading your dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType,\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Union\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ Libraries imported!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96750ab2-2959-4a7c-846e-c3d5038468c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration set!\n",
      "üìä Model: snorbyte/snorTTS-Indic-v0\n",
      "üìä Dataset: neuralmaverick47/FULL_ML_DATASET-2024-25-reset\n",
      "üíæ Output: ./snortts-indic-malayalam-finetuned\n",
      "üéµ Sample rate: 24000 Hz\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 3: Configuration Settings\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Model - snorbyte/snorTTS-Indic-v0\n",
    "    \"model_name\": \"snorbyte/snorTTS-Indic-v0\",\n",
    "    \n",
    "    # Your Malayalam dataset\n",
    "    \"dataset_name\": \"neuralmaverick47/FULL_ML_DATASET-2024-25-reset\",\n",
    "    \n",
    "    # Where to save the fine-tuned model\n",
    "    \"output_dir\": \"./snortts-indic-malayalam-finetuned\",\n",
    "    \n",
    "    # Training duration\n",
    "    \"num_train_epochs\": 15,\n",
    "    \n",
    "    # Batch settings\n",
    "    \"batch_size\": 2,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    \n",
    "    # Learning rate\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \n",
    "    # Logging and saving\n",
    "    \"warmup_steps\": 50,\n",
    "    \"logging_steps\": 10,\n",
    "    \"save_steps\": 200,\n",
    "    \"eval_steps\": 200,\n",
    "    \n",
    "    # Audio settings\n",
    "    \"max_text_length\": 200,\n",
    "    \"audio_sample_rate\": 24000,  # SnorTTS-Indic uses 22.05kHz\n",
    "    \"max_audio_length\": 30,  # seconds\n",
    "    \n",
    "    # LoRA settings\n",
    "    \"use_lora\": True,\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Configuration set!\")\n",
    "print(f\"üìä Model: {CONFIG['model_name']}\")\n",
    "print(f\"üìä Dataset: {CONFIG['dataset_name']}\")\n",
    "print(f\"üíæ Output: {CONFIG['output_dir']}\")\n",
    "print(f\"üéµ Sample rate: {CONFIG['audio_sample_rate']} Hz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24c30f05-ed1f-4bbe-977e-8cee54832b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä LOADING DATASET\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration neuralmaverick47--FULL_ML_DATASET-2024-25-reset-e24e113030060bc5\n",
      "Reusing dataset parquet (/root/.cache/huggingface/datasets/neuralmaverick47___parquet/neuralmaverick47--FULL_ML_DATASET-2024-25-reset-e24e113030060bc5/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230ab8bffda144b6ae5757842a136309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/neuralmaverick47___parquet/neuralmaverick47--FULL_ML_DATASET-2024-25-reset-e24e113030060bc5/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-ed7d0aa01e5f518d.arrow and /root/.cache/huggingface/datasets/neuralmaverick47___parquet/neuralmaverick47--FULL_ML_DATASET-2024-25-reset-e24e113030060bc5/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901/cache-55f98666d11cd286.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Available splits: ['train']\n",
      "‚úÖ Created 95/5 train/validation split\n",
      "\n",
      "üìä Training examples: 3021\n",
      "üìä Validation examples: 159\n",
      "\n",
      "üìã Dataset structure:\n",
      "Dataset({\n",
      "    features: ['index', 'audio', 'transcription'],\n",
      "    num_rows: 3021\n",
      "})\n",
      "\n",
      "üìÑ Sample entry:\n",
      "{'index': 2557, 'audio': {'path': 'chunk_21.wav', 'array': array([-0.00015259, -0.00015259, -0.00018311, ..., -0.00180054,\n",
      "       -0.00088501,  0.00036621]), 'sampling_rate': 16000}, 'transcription': '‡¥à ‡¥á‡¥™‡µç‡¥™‡µã‡µæ ‡¥ö‡µÜ‡¥Ø‡µç‡¥§ ‡¥ï‡¥æ‡¥∞‡µç‡¥Ø‡¥ô‡µç‡¥ô‡µæ ‡¥í‡¥ï‡µç‡¥ï‡µÜ ‡¥é‡¥≤‡µç‡¥≤‡¥æ‡¥∞‡µÅ‡¥Ç ‡¥í‡¥®‡µç‡¥®‡µç ‡¥ü‡µç‡¥∞‡µà ‡¥ö‡µÜ‡¥Ø‡µç‡¥§‡µç ‡¥®‡µã‡¥ï‡µç‡¥ï‡¥£‡¥Ç.'}\n",
      "\n",
      "üîç Column names: ['index', 'audio', 'transcription']\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 4: Load Dataset\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä LOADING DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "dataset = load_dataset(CONFIG[\"dataset_name\"])\n",
    "\n",
    "# Auto-detect splits\n",
    "available_splits = list(dataset.keys())\n",
    "print(f\"‚úÖ Available splits: {available_splits}\")\n",
    "\n",
    "# Use train/validation split\n",
    "if 'train' in available_splits:\n",
    "    train_data = dataset['train']\n",
    "    \n",
    "    if 'validation' in available_splits:\n",
    "        eval_data = dataset['validation']\n",
    "        print(\"‚úÖ Using existing train/validation splits\")\n",
    "    elif 'test' in available_splits:\n",
    "        eval_data = dataset['test']\n",
    "        print(\"‚úÖ Using train/test splits\")\n",
    "    else:\n",
    "        # Create validation split\n",
    "        split_dataset = train_data.train_test_split(test_size=0.05, seed=42)\n",
    "        train_data = split_dataset['train']\n",
    "        eval_data = split_dataset['test']\n",
    "        print(\"‚úÖ Created 95/5 train/validation split\")\n",
    "else:\n",
    "    train_data = dataset[available_splits[0]]\n",
    "    eval_data = None\n",
    "    print(f\"‚ö†Ô∏è Using '{available_splits[0]}' for training only\")\n",
    "\n",
    "print(f\"\\nüìä Training examples: {len(train_data)}\")\n",
    "if eval_data:\n",
    "    print(f\"üìä Validation examples: {len(eval_data)}\")\n",
    "\n",
    "print(f\"\\nüìã Dataset structure:\")\n",
    "print(train_data)\n",
    "print(f\"\\nüìÑ Sample entry:\")\n",
    "print(train_data[0])\n",
    "print(f\"\\nüîç Column names: {train_data.column_names}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01f9d090-db83-42d4-b902-b2294ed1a58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìù LOADING TOKENIZER AND FEATURE EXTRACTOR\n",
      "============================================================\n",
      "Loading tokenizer...\n",
      "‚úÖ Tokenizer loaded!\n",
      "   Vocabulary size: 156940\n",
      "   Pad token: <custom_token_7>\n",
      "\n",
      "Loading feature extractor...\n",
      "‚ö†Ô∏è Feature extractor not available: Can't load feature extractor for 'snorbyte/snorTTS-Indic-v0'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'snorbyte/snorTTS-Indic-v0' is the correct path to a directory containing a preprocessor_config.json file\n",
      "   Using default audio processing settings\n",
      "\n",
      "üß™ Test tokenization:\n",
      "   Input: '‡¥®‡¥Æ‡¥∏‡µç‡¥ï‡¥æ‡¥∞‡¥Ç'\n",
      "   Token IDs shape: torch.Size([1, 15])\n",
      "   First 10 tokens: [128000, 34839, 101, 34839, 106, 34839, 116, 85805, 243, 34839]\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 5: Load Tokenizer and Feature Extractor\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìù LOADING TOKENIZER AND FEATURE EXTRACTOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load tokenizer for text\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    CONFIG[\"model_name\"],\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"‚úÖ Tokenizer loaded!\")\n",
    "print(f\"   Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"   Pad token: {tokenizer.pad_token}\")\n",
    "\n",
    "# Load feature extractor for audio (if available)\n",
    "print(\"\\nLoading feature extractor...\")\n",
    "try:\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    print(\"‚úÖ Feature extractor loaded!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Feature extractor not available: {e}\")\n",
    "    print(\"   Using default audio processing settings\")\n",
    "    feature_extractor = None\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"‡¥®‡¥Æ‡¥∏‡µç‡¥ï‡¥æ‡¥∞‡¥Ç\"\n",
    "test_tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
    "print(f\"\\nüß™ Test tokenization:\")\n",
    "print(f\"   Input: '{test_text}'\")\n",
    "print(f\"   Token IDs shape: {test_tokens['input_ids'].shape}\")\n",
    "print(f\"   First 10 tokens: {test_tokens['input_ids'][0][:10].tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a55fe359-436c-4046-bf91-426226d39edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéµ PREPARING AUDIO DATA\n",
      "============================================================\n",
      "Setting audio sample rate to 24000 Hz...\n",
      "‚úÖ Audio will be processed during dataset preparation\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 6: Prepare Audio Data\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéµ PREPARING AUDIO DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cast audio to correct sample rate\n",
    "print(f\"Setting audio sample rate to {CONFIG['audio_sample_rate']} Hz...\")\n",
    "\n",
    "# Note: We'll handle audio processing in the preprocessing function\n",
    "print(f\"‚úÖ Audio will be processed during dataset preparation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23d787af-3fb6-46bf-a245-276dbefcc53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x7ffac23f47c0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa5fceb86fb04849984b1012a9ef1f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üöÄ GPU-Accelerated SNAC Encoding:   0%|          | 0/95 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Final Processed Dataset Count: 3021\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from snac import SNAC  # Use the correct class name\n",
    "\n",
    "# 1. Properly load the SNAC model\n",
    "# If you don't load the weights here, snac_model is undefined\n",
    "snac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\").to(\"cuda\")\n",
    "snac_model.eval()\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    all_input_ids = []\n",
    "    \n",
    "    # Identify transcription column\n",
    "    text_col = next((c for c in ['transcription', 'text', 'sentence'] if c in examples), None)\n",
    "    \n",
    "    if text_col is None:\n",
    "        print(\"‚ùå Error: Could not find text column in dataset.\")\n",
    "        return {\"input_ids\": [], \"labels\": []}\n",
    "\n",
    "    for i in range(len(examples[text_col])):\n",
    "        try:\n",
    "            # A. Text to Tokens\n",
    "            text = examples[text_col][i]\n",
    "            # Ensure tokenizer is defined in your environment\n",
    "            text_tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "            \n",
    "            # B. Audio Processing\n",
    "            audio_data = examples['audio'][i]\n",
    "            waveform = np.array(audio_data['array'])\n",
    "            sr = audio_data['sampling_rate']\n",
    "            \n",
    "            # SNAC requires 24kHz\n",
    "            if sr != 24000:\n",
    "                waveform = librosa.resample(waveform, orig_sr=sr, target_sr=24000)\n",
    "            \n",
    "            # C. GPU-Accelerated Encoding\n",
    "            audio_tensor = torch.from_numpy(waveform).float().unsqueeze(0).unsqueeze(0).to(\"cuda\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                codes = snac_model.encode(audio_tensor)\n",
    "            \n",
    "            # D. Flattening & Offsetting (The 7-token pattern for snorTTS/Indic)\n",
    "            AUDIO_OFFSET = 128266 \n",
    "            flat_audio_tokens = []\n",
    "            for level in codes:\n",
    "                # level is [1, T]. Squeeze to get 1D and add offset\n",
    "                tokens = (level.squeeze(0).cpu().numpy() + AUDIO_OFFSET).astype(int).tolist()\n",
    "                flat_audio_tokens.extend(tokens)\n",
    "            \n",
    "            # E. Concatenation\n",
    "            combined_sequence = text_tokens + flat_audio_tokens\n",
    "            all_input_ids.append(combined_sequence[:2048])\n",
    "            \n",
    "        except Exception as e:\n",
    "            # DON'T just continue; print the first few errors to debug\n",
    "            if i < 3: \n",
    "                print(f\"‚ö†Ô∏è Error at index {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return {\"input_ids\": all_input_ids, \"labels\": all_input_ids}\n",
    "\n",
    "# 2. Execute\n",
    "# Try batch_size=1 first if it keeps returning zero to see the error messages\n",
    "processed_dataset = train_data.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    batch_size=32, \n",
    "    remove_columns=train_data.column_names,\n",
    "    desc=\"üöÄ GPU-Accelerated SNAC Encoding\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Final Processed Dataset Count: {len(processed_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "975fe6d0-b598-4749-a467-5440b787324b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üì¶ INITIALIZING DATA COLLATOR\n",
      "============================================================\n",
      "‚úÖ Data Collator initialized using standard CLM logic.\n",
      "üí° No audio padding needed; the collator now pads integer tokens.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BLOCK 9: Data Collator (LSM / Llama-3 Optimized)\n",
    "# ============================================================================\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üì¶ INITIALIZING DATA COLLATOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Since snorTTS is a Causal LLM (Llama-based), we treat everything as \n",
    "# a language modeling task. \n",
    "# mlm=False tells the collator we are doing Causal LM (next token prediction), \n",
    "# not Masked LM (like BERT).\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Important: This is for Causal LM (Llama)\n",
    "    pad_to_multiple_of=8 # Optimized for Shakti Cloud GPUs (H100/L40S)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Data Collator initialized using standard CLM logic.\")\n",
    "print(\"üí° No audio padding needed; the collator now pads integer tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c217a8-51bf-46af-aeb9-edc1f7716a18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ü§ñ LOADING SNORTTS-INDIC (FINAL STABLE MODE)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b4e9ac779143b4a7e182d4e22648f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üìç Real Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BLOCK 10: Load snorTTS (Meta-Tensor Fix & Stable Memory)\n",
    "# ============================================================================\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ü§ñ LOADING SNORTTS-INDIC (FINAL STABLE MODE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "offload_dir = \"model_offload_cache\"\n",
    "os.makedirs(offload_dir, exist_ok=True)\n",
    "\n",
    "# 1. Standardized Quantization Config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16, \n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    # 2. Load without manual config overrides inside from_pretrained\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        CONFIG[\"model_name\"],\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        token=\"\",#place hf token\n",
    "        offload_folder=offload_dir,\n",
    "        torch_dtype=torch.float16, # Prevents meta-tensor type mismatch\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    # 3. CRITICAL: Only set config values AFTER model is loaded on real devices\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    # Check if pad_token exists, if not, align with tokenizer\n",
    "    if tokenizer.pad_token_id is not None:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"üìç Real Device: {model.device}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Load failed: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"If the error persists, try removing device_map='auto' and use device_map={'': 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a872935-21a1-44c6-afb0-fdf97fc819a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üéØ APPLYING LORA FOR SNORTTS (LLAMA-3.2 BACKBONE)\n",
      "============================================================\n",
      "‚úÖ LoRA Configured for Llama-3.2 Decoder-Only Architecture.\n",
      "üìä Targeted Modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "trainable params: 24,313,856 || all params: 3,325,180,928 || trainable%: 0.7312\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# BLOCK 11: Apply LoRA for Llama-3.2 (Causal LM Architecture)\n",
    "# ============================================================================\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "if CONFIG[\"use_lora\"]:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ APPLYING LORA FOR SNORTTS (LLAMA-3.2 BACKBONE)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Prepare model for k-bit training (Essential for 4-bit/8-bit models)\n",
    "    # This enables gradient checkpointing and ensures non-trainable weights are frozen.\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    \n",
    "    # 2. Target Modules for Llama-3.2\n",
    "    # For Llama-3 architectures, we target ALL linear layers to capture \n",
    "    # the nuances of Malayalam phonetics and tone.\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    "    \n",
    "    # 3. Configure LoRA for Causal Language Modeling\n",
    "    # Note: TaskType.CAUSAL_LM is mandatory for Llama-based models.\n",
    "    lora_config = LoraConfig(\n",
    "        r=CONFIG.get(\"lora_r\", 16),\n",
    "        lora_alpha=CONFIG.get(\"lora_alpha\", 32),\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM  # CRITICAL: Llama is a Causal LM\n",
    "    )\n",
    "    \n",
    "    # 4. Wrap model with LoRA adapters\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    # Enable gradient checkpointing to save massive amounts of VRAM\n",
    "    model.gradient_checkpointing_enable()\n",
    "    \n",
    "    print(\"‚úÖ LoRA Configured for Llama-3.2 Decoder-Only Architecture.\")\n",
    "    print(f\"üìä Targeted Modules: {target_modules}\")\n",
    "    model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf505e0-c046-4d0e-b0c2-407033bdcdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BLOCK 12: Training Configuration\n",
    "# ============================================================================\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚öôÔ∏è CONFIGURING TRAINING SETTINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CONFIG[\"output_dir\"],\n",
    "    num_train_epochs=CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=1, # Lowest for VRAM stability\n",
    "    gradient_accumulation_steps=16, # Effective batch size = 16\n",
    "    learning_rate=5e-5,            # Stable for audio token prediction\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    fp16=True,                     # Use mixed precision\n",
    "    gradient_checkpointing=True,   # Saves massive VRAM\n",
    "    optim=\"paged_adamw_8bit\",      # Stable 8-bit optimizer\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf30ff9-13fe-4e5a-b9bb-add836178cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BLOCK 13 & 14: Trainer Initialization & Execution\n",
    "# ============================================================================\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset, # The SNAC-encoded stream\n",
    "    data_collator=data_collator,     # The CLM collator from Block 9\n",
    ")\n",
    "\n",
    "print(\"\\nüöÄ STARTING FINE-TUNING (MALAYALAM SPEECH)\")\n",
    "print(\"=\"*60)\n",
    "trainer.train()\n",
    "print(\"\\nüéâ TRAINING COMPLETE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6240344-606f-4260-8f91-8a3cd07463d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_malayalam_speech(\"‡¥®‡¥Æ‡¥∏‡µç‡¥ï‡¥æ‡¥∞‡¥Ç, ‡¥∏‡µÅ‡¥ñ‡¥Æ‡¥æ‡¥£‡µã?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e218016-bd63-468c-84bf-da03f4d0ebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-13 10:17:48.469\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1müì¶ Loading model in BF16 stability mode...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba3de3a358347d2aa1ad94840719fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2026-01-13 10:18:05.476\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m52\u001b[0m - \u001b[32m\u001b[1m‚úÖ Model loaded successfully in BF16!\u001b[0m\n",
      "\u001b[32m2026-01-13 10:18:16.592\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m58\u001b[0m - \u001b[32m\u001b[1m‚úÖ Sequential model loaded successfully.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully saved audio locally to: /workspace/malayalam_xtts_training/malayalam_tts_output/29-DEC-TTS/snorTTS-Indic-v0/malayalam_final_output.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from snac import SNAC\n",
    "from loguru import logger\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ============================================================================\n",
    "# Use the SnorTTS model as the \"Base\" for your sequential fine-tune\n",
    "BASE_MODEL_ID = 'snorbyte/snorTTS-Indic-v0'\n",
    "# Path to your specific Malayalam checkpoint folder\n",
    "CHECKPOINT_PATH = 'snortts-indic-malayalam-finetuned/checkpoint-2600'\n",
    "HUGGINGFACE_TOKEN = \"\" # Add your token \n",
    "\n",
    "# SnorTTS Architecture Constants\n",
    "AUDIO_START_ID = 128266\n",
    "END_OF_SPEECH_ID = 128258\n",
    "MAX_SEQ_LENGTH = 2048\n",
    "\n",
    "# ============================================================================\n",
    "# 2. SEQUENTIAL MODEL LOADING\n",
    "# ============================================================================\n",
    "# Restart Kernel before running this!\n",
    "logger.info(\"üì¶ Loading model in BF16 stability mode...\")\n",
    "\n",
    "# 1. Skip bnb_config entirely to avoid the quantization crash\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT_PATH)\n",
    "\n",
    "# 2. Load base model in Bfloat16 (Standard for Llama 3.2 / H100)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16, # Use 16-bit instead of 4-bit\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    token=HUGGINGFACE_TOKEN,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 3. Load your Malayalam adapters\n",
    "model = PeftModel.from_pretrained(base_model, CHECKPOINT_PATH)\n",
    "model.eval()\n",
    "logger.success(\"‚úÖ Model loaded successfully in BF16!\")\n",
    "\n",
    "# Load and Attach your fine-tuned Malayalam adapters\n",
    "# Removed 'token=HUGGINGFACE_TOKEN' as it's a local path\n",
    "model = PeftModel.from_pretrained(base_model, CHECKPOINT_PATH)\n",
    "model.eval()\n",
    "logger.success(\"‚úÖ Sequential model loaded successfully.\")\n",
    "\n",
    "# Load SNAC decoder (24kHz)\n",
    "snac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\").to(\"cuda\").eval()\n",
    "\n",
    "# ============================================================================\n",
    "# 3. GENERATION FUNCTION\n",
    "# ============================================================================\n",
    "def generate_malayalam_audio(text, speaker_id=189):\n",
    "    prompt = f\"<custom_token_3><|begin_of_text|>malayalam{speaker_id}: {text} <|eot_id|><custom_token_4><custom_token_5><custom_token_1>\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=1024,\n",
    "            temperature=0.5,\n",
    "            do_sample=True,\n",
    "            eos_token_id=END_OF_SPEECH_ID,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # 1. Extraction\n",
    "    generated_ids = output[0][inputs.input_ids.shape[1]:].cpu().tolist()\n",
    "    audio_ids = [tid for tid in generated_ids if tid >= AUDIO_START_ID]\n",
    "    \n",
    "    num_frames = len(audio_ids) // 7\n",
    "    if num_frames == 0:\n",
    "        return None\n",
    "\n",
    "    # 2. THE SHAKTHI FIX: Strict Bounds Checking\n",
    "    # We ensure every ID is between 0 and 4095 before passing to SNAC\n",
    "    clean_ids = [tid - AUDIO_START_ID for tid in audio_ids[:num_frames * 7]]\n",
    "    l1, l2, l3 = [], [], []\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        idx = i * 7\n",
    "        try:\n",
    "            # Level 1\n",
    "            l1.append(max(0, min(4095, clean_ids[idx])))\n",
    "            # Level 2\n",
    "            l2.append(max(0, min(4095, clean_ids[idx + 1] - 4096)))\n",
    "            l2.append(max(0, min(4095, clean_ids[idx + 4] - (4 * 4096))))\n",
    "            # Level 3\n",
    "            l3.append(max(0, min(4095, clean_ids[idx + 2] - (2 * 4096))))\n",
    "            l3.append(max(0, min(4095, clean_ids[idx + 3] - (3 * 4096))))\n",
    "            l3.append(max(0, min(4095, clean_ids[idx + 5] - (5 * 4096))))\n",
    "            l3.append(max(0, min(4095, clean_ids[idx + 6] - (6 * 4096))))\n",
    "        except IndexError:\n",
    "            break\n",
    "\n",
    "    # 3. Safe Tensors\n",
    "    codes = [torch.tensor(c).unsqueeze(0).to(\"cuda\") for c in [l1, l2, l3]]\n",
    "\n",
    "    # 4. Final Reconstruction\n",
    "    with torch.inference_mode():\n",
    "        # This is where the assert usually happens if 'codes' has invalid values\n",
    "        audio_waveform = snac_model.decode(codes)\n",
    "    \n",
    "    return audio_waveform.detach().squeeze().cpu().numpy()\n",
    "\n",
    "# ============================================================================\n",
    "# 4. RUN\n",
    "# ============================================================================\n",
    "text = \"‡¥®‡¥Æ‡¥∏‡µç‡¥ï‡¥æ‡¥∞‡¥Ç, ‡¥á‡¥§‡µç ‡¥í‡¥∞‡µÅ ‡¥∏‡µÜ‡¥ï‡µç‡¥µ‡µª‡¥∑‡µç‡¥Ø‡µΩ ‡¥´‡µà‡µª ‡¥ü‡µç‡¥Ø‡µÇ‡µ∫‡¥°‡µç ‡¥Æ‡µã‡¥°‡¥≤‡¥æ‡¥£‡µç.\"\n",
    "audio = generate_malayalam_audio(text)\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "if audio is not None:\n",
    "    # 2. Define your local path\n",
    "    # You can use a relative path like \"output.wav\" or an absolute path\n",
    "    output_path = \"malayalam_final_output.wav\"\n",
    "    \n",
    "    # 3. Save the file using soundfile\n",
    "    # We cast to float32 to ensure compatibility with most players\n",
    "    sf.write(output_path, audio.astype(np.float32), 24000)\n",
    "    \n",
    "    print(f\"‚úÖ Successfully saved audio locally to: {os.path.abspath(output_path)}\")\n",
    "else:\n",
    "    print(\"‚ùå Audio generation failed, no file was saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a86d37-a346-4c3f-8262-1c9c287e1934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
